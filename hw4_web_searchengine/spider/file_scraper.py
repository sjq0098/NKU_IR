import os
import time
import random
import logging
import requests
from datetime import datetime
from urllib.parse import urljoin, urlparse, unquote
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from pymongo import MongoClient, errors
import gridfs

import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning

# å…¨å±€å…³é—­ InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("multi_domain_file_scraper.log", encoding='utf-8')
    ]
)

# å¯ä¸‹è½½æ–‡ä»¶åç¼€
DOWNLOAD_SUFFIXES = {".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
                     ".mp3", ".mp4", ".avi", ".mkv", ".mov", ".wmv", ".flv",
                     ".zip", ".rar", ".tar", ".gz", ".bz2", ".7z",
                     ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".tiff",
                     ".csv", ".txt", ".rtf"}

# è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# æœ€å¤§ä¸‹è½½æ–‡ä»¶æ•°
MAX_FILES = 1000
# å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°
WORKERS = 8  # é™ä½å¹¶å‘æ•°é¿å…è¢«å°
# è¯·æ±‚å»¶è¿ŸèŒƒå›´
DELAY_RANGE = (1, 2)  # å¢åŠ å»¶è¿Ÿé¿å…è¢«åçˆ¬è™«

class MultiDomainFileScraper:
    def __init__(self, seeds):
        self.seeds = seeds
        self.visited = set()
        self.to_crawl = list(seeds)
        self.downloaded_count = 0

        # æµ‹è¯•å¹¶åˆå§‹åŒ–MongoDBè¿æ¥
        try:
            self.client = MongoClient("mongodb://localhost:27017/", serverSelectionTimeoutMS=5000)
            # æµ‹è¯•è¿æ¥
            self.client.admin.command('ismaster')
            self.db = self.client['nankai_news_datasets']
            self.fs = gridfs.GridFS(self.db)
            self.files_col = self.db['FILES']
            self.files_col.create_index('url', unique=True)
            logging.info("âœ… MongoDBè¿æ¥æˆåŠŸ")
        except errors.ServerSelectionTimeoutError:
            logging.error("âŒ MongoDBè¿æ¥å¤±è´¥ï¼è¯·ç¡®ä¿MongoDBæœåŠ¡å·²å¯åŠ¨")
            raise Exception("MongoDBè¿æ¥å¤±è´¥")
        except Exception as e:
            logging.error(f"âŒ MongoDBåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

        # å…è®¸çš„åŸŸåé›†åˆ
        self.allowed_domains = {urlparse(u).netloc for u in seeds}
        logging.info(f"ğŸ¯ å°†çˆ¬å–ä»¥ä¸‹åŸŸå: {self.allowed_domains}")

    def fetch_soup(self, url):
        try:
            time.sleep(random.uniform(*DELAY_RANGE))
            response = requests.get(url, headers=HEADERS, timeout=15, verify=False)
            response.encoding = response.apparent_encoding
            
            if response.status_code == 200 and 'text/html' in response.headers.get('Content-Type', ''):
                logging.info(f"âœ… æˆåŠŸè·å–é¡µé¢: {url}")
                return BeautifulSoup(response.text, 'html.parser')
            else:
                logging.warning(f"âš ï¸ é¡µé¢å“åº”å¼‚å¸¸ {response.status_code}: {url}")
        except requests.exceptions.Timeout:
            logging.warning(f"â° è¯·æ±‚è¶…æ—¶: {url}")
        except requests.exceptions.ConnectionError:
            logging.warning(f"ğŸ”Œ è¿æ¥é”™è¯¯: {url}")
        except Exception as e:
            logging.warning(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
        return None

    def extract_links(self, soup, base_url):
        """æå–é¡µé¢ä¸­æ‰€æœ‰åˆæ³•é“¾æ¥åŠå…¶æ ‡é¢˜"""
        links = []
        if not soup:
            return links
            
        for a in soup.find_all('a', href=True):
            try:
                href = a['href'].strip()
                if not href:
                    continue
                    
                full_url = urljoin(base_url, href)
                parsed = urlparse(full_url)
                
                # åªå¤„ç†å…è®¸çš„åŸŸå
                if parsed.netloc not in self.allowed_domains:
                    continue
                
                # è·å–é“¾æ¥æ ‡é¢˜
                title = a.get('title', '').strip()
                if not title:
                    title = a.get_text(strip=True)
                if not title:
                    title = unquote(os.path.basename(parsed.path))
                
                title = title.split('?')[0].split('#')[0].strip()
                if title:
                    links.append((full_url, title))
            except Exception as e:
                logging.debug(f"è§£æé“¾æ¥å¤±è´¥: {e}")
                continue
                
        logging.info(f"ğŸ“‹ ä» {base_url} æå–åˆ° {len(links)} ä¸ªé“¾æ¥")
        return links

    def is_download_link(self, url):
        """æ£€æŸ¥æ˜¯å¦ä¸ºå¯ä¸‹è½½æ–‡ä»¶é“¾æ¥"""
        try:
            parsed_url = urlparse(url.lower())
            path = parsed_url.path
            return any(path.endswith(suffix) for suffix in DOWNLOAD_SUFFIXES)
        except:
            return False

    def download_and_store(self, url, title):
        """ä¸‹è½½å¹¶å­˜å‚¨æ–‡ä»¶åˆ°MongoDB GridFS"""
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if self.files_col.find_one({'url': url}):
            logging.info(f"â­ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {url}")
            return False
            
        try:
            time.sleep(random.uniform(*DELAY_RANGE))
            logging.info(f"â¬‡ï¸ å¼€å§‹ä¸‹è½½: {url}")
            
            response = requests.get(url, headers=HEADERS, timeout=30, verify=False, stream=True)
            if response.status_code == 200:
                # å°è¯•ä»å“åº”å¤´è·å–æ–‡ä»¶å
                content_disposition = response.headers.get('Content-Disposition', '')
                if 'filename=' in content_disposition:
                    filename = content_disposition.split('filename=')[-1].strip('"\'')
                    if filename:
                        title = filename
                
                # è·å–æ–‡ä»¶å†…å®¹
                content = response.content
                if len(content) == 0:
                    logging.warning(f"âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©º: {url}")
                    return False
                
                # å­˜å‚¨åˆ°GridFS
                file_id = self.fs.put(
                    content,
                    filename=os.path.basename(title) or f"file_{int(time.time())}",
                    url=url,
                    title=title,
                    upload_date=datetime.utcnow(),
                    file_size=len(content)
                )
                
                # å­˜å‚¨å…ƒæ•°æ®
                file_info = {
                    'url': url,
                    'title': title,
                    'file_name': os.path.basename(title) or f"file_{int(time.time())}",
                    'file_type': os.path.splitext(title)[1].lstrip('.') if '.' in title else 'unknown',
                    'file_size': len(content),
                    'gridfs_id': file_id,
                    'fetched_at': datetime.utcnow()
                }
                
                self.files_col.insert_one(file_info)
                self.downloaded_count += 1
                logging.info(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ ({self.downloaded_count}): {title} - {len(content)} bytes")
                return True
            else:
                logging.warning(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç  {response.status_code}: {url}")
                
        except requests.exceptions.Timeout:
            logging.warning(f"â° ä¸‹è½½è¶…æ—¶: {url}")
        except requests.exceptions.ConnectionError:
            logging.warning(f"ğŸ”Œ ä¸‹è½½è¿æ¥é”™è¯¯: {url}")
        except Exception as e:
            logging.error(f"âŒ ä¸‹è½½å¼‚å¸¸ {url}: {e}")
        
        return False

    def run(self):
        """ä¸»è¿è¡Œæ–¹æ³•"""
        logging.info(f"ğŸš€ å¼€å§‹çˆ¬å–ï¼Œç›®æ ‡æ–‡ä»¶æ•°: {MAX_FILES}")
        
        # è·å–å·²ä¸‹è½½æ–‡ä»¶æ•°
        existing_files = self.files_col.count_documents({})
        logging.info(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰ {existing_files} ä¸ªæ–‡ä»¶")
        
        processed_pages = 0
        download_futures = []
        
        with ThreadPoolExecutor(max_workers=WORKERS) as executor:
            while self.to_crawl and (existing_files + self.downloaded_count) < MAX_FILES:
                # å¤„ç†å½“å‰é¡µé¢
                current_url = self.to_crawl.pop(0)
                if current_url in self.visited:
                    continue
                    
                self.visited.add(current_url)
                processed_pages += 1
                
                logging.info(f"ğŸ” æ­£åœ¨å¤„ç†ç¬¬ {processed_pages} ä¸ªé¡µé¢: {current_url}")
                
                # è·å–é¡µé¢å†…å®¹
                soup = self.fetch_soup(current_url)
                if not soup:
                    continue
                
                # æå–é“¾æ¥
                links = self.extract_links(soup, current_url)
                
                download_links = []
                page_links = []
                
                for link_url, link_title in links:
                    if self.is_download_link(link_url):
                        download_links.append((link_url, link_title))
                    else:
                        if link_url not in self.visited:
                            page_links.append(link_url)
                
                # æäº¤ä¸‹è½½ä»»åŠ¡
                for download_url, download_title in download_links:
                    if (existing_files + self.downloaded_count + len(download_futures)) >= MAX_FILES:
                        break
                    future = executor.submit(self.download_and_store, download_url, download_title)
                    download_futures.append(future)
                
                # æ·»åŠ æ–°çš„é¡µé¢é“¾æ¥åˆ°å¾…çˆ¬å–é˜Ÿåˆ—
                self.to_crawl.extend(page_links)
                
                logging.info(f"ğŸ“ˆ æœ¬é¡µæ‰¾åˆ° {len(download_links)} ä¸ªä¸‹è½½é“¾æ¥ï¼Œ{len(page_links)} ä¸ªé¡µé¢é“¾æ¥")
                
                # æ£€æŸ¥å¹¶æ”¶é›†å·²å®Œæˆçš„ä¸‹è½½ä»»åŠ¡
                completed_futures = []
                for future in download_futures:
                    if future.done():
                        completed_futures.append(future)
                        try:
                            future.result()  # è·å–ç»“æœï¼Œå¦‚æœæœ‰å¼‚å¸¸ä¼šæŠ›å‡º
                        except Exception as e:
                            logging.error(f"âŒ ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
                
                # ç§»é™¤å·²å®Œæˆçš„future
                for completed_future in completed_futures:
                    download_futures.remove(completed_future)
                
                # çŠ¶æ€æŠ¥å‘Š
                if processed_pages % 10 == 0:
                    total_downloaded = existing_files + self.downloaded_count
                    logging.info(f"ğŸ“Š è¿›åº¦æŠ¥å‘Š: å·²å¤„ç† {processed_pages} é¡µï¼Œå·²ä¸‹è½½ {total_downloaded} æ–‡ä»¶ï¼Œé˜Ÿåˆ—ä¸­è¿˜æœ‰ {len(self.to_crawl)} é¡µé¢ï¼Œ{len(download_futures)} ä¸ªä¸‹è½½ä»»åŠ¡")
            
            # ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ
            logging.info("â³ ç­‰å¾…å‰©ä½™ä¸‹è½½ä»»åŠ¡å®Œæˆ...")
            for future in as_completed(download_futures):
                try:
                    future.result()
                except Exception as e:
                    logging.error(f"âŒ æœ€ç»ˆä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")

        total_downloaded = existing_files + self.downloaded_count
        logging.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼æ€»å…±ä¸‹è½½äº† {total_downloaded} ä¸ªæ–‡ä»¶ï¼ˆæœ¬æ¬¡æ–°å¢ {self.downloaded_count} ä¸ªï¼‰")
        self.client.close()

if __name__ == '__main__':
    # ç§å­URL
    seeds = [
        "https://jwc.nankai.edu.cn",
        "https://zsb.nankai.edu.cn", 
        "https://lib.nankai.edu.cn",
        "https://cc.nankai.edu.cn",
        "https://cs.nankai.edu.cn",
        "https://ai.nankai.edu.cn",
        "https://finance.nankai.edu.cn",
        "https://math.nankai.edu.cn",
        "https://physics.nankai.edu.cn",
        "https://chem.nankai.edu.cn",
        "https://economics.nankai.edu.cn",
        "https://bs.nankai.edu.cn"
    ]
    
    try:
        scraper = MultiDomainFileScraper(seeds)
        scraper.run()
    except KeyboardInterrupt:
        logging.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logging.error(f"ğŸ’¥ ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
